{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "오늘 강의에서 Dropout regularization이 추가된 Neural networks (dropout neural network)는 variational inference를 이용한 Bayesian neural networks로 해석할 수 있다는 사실을 배웠습니다. 첫 번째 실습에서 dropout neural network의 stochastic output을 이용하여 neural network의 uncertainty를 측정하는 방법에 대해 배웠습니다. 우리가 보통 사용하는 dropout은 각 layer의 모든 (input 또는 hidden) unit들이 같은 dropout rate을 갖도록 합니다. 하지만, 각 layer의 각 unit에 적합한 dropout rate을 일일이 찾는다는 건 불가능에 가깝기 때문에 unit이 각자의 dropout rate을 학습하도록 하면 더 나은 model uncertainty를 얻을 수 있으며 더 좋은 performance도 얻을 수 있을 것입니다. 이번 실습에서는 그것을 가능케 하는 방법 중 하나인 concrete dropout에 대해서 배우고자 합니다.\n",
    "\n",
    "이번 실습에서 우리는 다음과 같은 내용을 학습할 예정입니다.\n",
    "* Bayesian inference & Variational inference Review\n",
    "* Concrete dropout에 사용되는 중요한 개념인 Gumbel-softmax trick을 이용한 Concrete distribution\n",
    "* Concrete distribution을 통해 각 (input 또는 hidden) unit들의 dropout rate을 학습할 수 있는 concrete dropout\n",
    "* Concrete dropout을 구성하는 요소들에 대한 이해 및 구현\n",
    "* 구현한 concrete dropout neural network를 이용하여 각 unit들의 dropout rate 변화 관찰\n",
    "\n",
    "## Bayesian Inference and Variational Inference (VI)\n",
    "\n",
    "오전 강의와 앞서 첫 번째 실습에서도 Review했듯이 Bayesian inference의 기본 과정은 다음과 같이 prior를 설정해주고 데이터에 따른 posterior를 구하는 것입니다.\n",
    "\n",
    "![bayesian](images/bayesian_inference.png)\n",
    "\n",
    "그런데 $W$가 neural network의 weight parameter인 경우에는 posterior distribution $p(W|\\mathcal{D})$를 정확히 계산하기란 불가능합니다. 따라서, variational inference 관점에서는 true posterior를 잘 묘사할 수 있는 또는 계산하기 상대적으로 편한 \"variational distribution\" $q_{\\phi}(W)$를 설정하고 (ex. mean-field Gaussian) true posterior와 variational approximate distribution 사이의 거리를 줄이는 문제로 치환하게 됩니다. 여기서 두 distribution 사이의 거리를 측정할 때는 흔히 알고 있는 KL divergence를 이용합니다. 조금 더 자세히 식을 정리해보면\n",
    "\n",
    "![elbo](images/elbo.png)\n",
    "\n",
    "위와 같이 나타낼 수 있습니다. Prior를 우리가 설정해주듯이 variational distribution 또한 우리가 optimize하기 쉬운 형태의 form으로 보통 설정해줍니다. 그래서 mean-field Gaussian distribution이 가장 많이 사용되는 예시입니다. 그럼 이제 dropout neural network가 어떻게 variational inference로 해석되는지 알아봅니다.\n",
    "\n",
    "## Dropout as a Variational Inference\n",
    "\n",
    "우리는 오전 강의에서 Dropout regularization이 추가된 Neural network는 Bayesian neural network로 볼 수 있다는 점을 배웠습니다. 또한, 그에 대한 example로 Concrete dropout, Variational dropout, CNN dropout 등을 살펴보았습니다. Dropout은 각각의 (input 또는 hidden) unit들이 Bernoulli distribution을 따르게 됩니다. 다음 그림은 dropout의 intuitive한 설명을 나타냅니다.\n",
    "\n",
    "![dropout](images/dropout.png)\n",
    "\n",
    "Dropout이 추가된 Neural network이 어떻게 variational inference로 해석될 수 있는지에 대해 알아보기 전에 Bayesian inference와 variational inference에 대해 review하도록 하겠습니다.\n",
    "\n",
    "Dropout neural network는 $l^{\\text{th}}$ layer의 weight parameter를 $W_l \\in \\mathbb{R}^{K_{l+1} \\times K_l}$이라 했을 때 prior $p(W_l)$는 Gaussian, posterior에 대한 variational distribution은 $q_{M_l}(W_l) = M_l\\cdot\\mathrm{diag}[\\mathrm{Bernoulli}(1-p_l)^{K_l}]$ 를 따르는 것과 동일하다는 것이 알려져 있습니다.\n",
    "\n",
    "![var_interp](images/interpretation.png)\n",
    "\n",
    "여기서 variational parameter $\\phi = \\{M_l, p_l\\}$이며 $M_l$은 $l^{\\text{th}}$ layer weight의 realization, $p_l$은 각 layer의 dropout rate을 나타냅니다. 자세한 derivation 과정은 생략하도록 하겠습니다. 이를 이용하면 위의 Evidence Lower BOund (ELBO) term에서 KL regularization term은 다음처럼 derive됩니다.\n",
    "\n",
    "![kl_divergence](images/kl_divergence.png)\n",
    "\n",
    "여기서 $\\mathcal{H}(p_l)$은\n",
    "\n",
    "![entropy](images/entropy.png)\n",
    "\n",
    "입니다.\n",
    "\n",
    "이는 하나의 $l^{\\text{th}}$ layer에만 해당되기 때문에 최종적으로 우리가 minimize해야 하는 term은\n",
    "\n",
    "![elbo2](images/elbo.png)\n",
    "\n",
    "에서 ELBO이며 우리는 첫 번째 KL term에 대한 derive를 위에서 끝냈습니다. 따라서,\n",
    "\n",
    "![final_objective](images/final_objective.png)\n",
    "\n",
    "과 같이 모든 layer에 대한 term을 더해주어야 합니다.\n",
    "\n",
    "에서 ELBO라고 적힌 Term입니다. ELBO의 첫 번째 Term인 KL divergence는 위에 derive한 식을 그대로 이용하면 되지만 두 번째 Term은 variational distribution을 이용한 sampling이 필요합니다.\n",
    "\n",
    "하지만, 여기서 문제점은 $p_l$은 discrete한 Bernoulli distribution의 parameter이기 때문에 우리가 일반적으로 사용하는 Backpropagation으로 학습하기 어렵습니다. 따라서, continuous하게 relaxation 해주어야 하며 이를 위해 concrete distribution에 대해 알아보도록 하겠습니다.\n",
    "\n",
    "## Concrete Distributions (Gumbel-Softmax Trick)\n",
    "\n",
    "Concrete distribution는 Bernoulli distribution 뿐만 아니라 categorical distribution을 갖는 any random variable에 적용 가능한 테크닉입니다. Categorical probability $(\\alpha_1, \\alpha_2, \\cdots, \\alpha_K)$를 갖는 random variable $Z$에 대해 sampling 하는 방법은 다음과 같은 Gumbel distribution 통해 나타낼 수 있습니다.\n",
    "\n",
    "\\begin{align*}\n",
    "    z = \\mathtt{one\\_hot} \\Big(\\arg\\max_i [G_i + \\log \\alpha_i]\\Big)\n",
    "\\end{align*}\n",
    "\n",
    "여기서 $G_i$는 $\\mathrm{Gumbel}(0,1)$ distribution을 따릅니다. 하지만, argmax operator가 non-differentiable하기 때문에 class probability $\\alpha_i$ 자체를 학습하고 싶은 경우에는 Backpropagation을 하는 데에 여전히 문제가 됩니다 (ex. concrete dropout처럼 dropout rate 자체도 학습하고 싶은 경우). 따라서, argmax operator를 우리가 잘 아는 softmax로 relaxation을 하면 문제는 해결됩니다. 이를 그림으로 나타내면 다음과 같습니다.\n",
    "\n",
    "![gumbel_softmax](images/gumbel_softmax.png)\n",
    "\n",
    "여기서 $\\lambda$는 continuous relaxation의 정도를 나타내는 parameter이며 이 값이 커질수록 approximate된 distribution은 조금 더 smooth한 형태를 띄게 됩니다. 다음은 조금 더 이해를 돕기 위한 그림입니다. \n",
    "\n",
    "![concrete_distributions](images/concrete_distributions.png)\n",
    "\n",
    "특수한 case로 Bernoulli random variable $Z$에 대해서는 다음과 같은 concrete distribution trick을 이용하여 sampling이 가능합니다.\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\mathrm{Sigmoid}\\Big((\\log u - \\log (1 - u) + \\log p - \\log (1 - p)) / \\lambda\\Big)\n",
    "\\end{align}\n",
    "\n",
    "여기서 $u$는 $\\mathrm{Uniform}(0,1)$ random variable입니다. tf.random.uniform과 같은 함수로 쉽게 sampling이 가능합니다. 따라서, 원래 dropout rate $p$ 자체를 sampling하는 작업이 uniform random variable $U$를 대신 sampling하는 것으로 치환됨으로써 dropout $p$ 역시 backpropagation을 이용하여 학습을 할 수 있습니다.\n",
    "\n",
    "조금 더 직관적인 설명을 위해 우리가 보통 variational distribution $q_{\\phi}(z)$를 $\\mathcal{N}(z|\\mu, \\sigma)$로 mean-field Gaussian으로 설정하는 경우를 생각해봅시다. Sampling 과정에서 Variational parameter $(\\mu, \\sigma)$가 직접적으로 연관이 되어 있기 때문에 $(\\mu, \\sigma)$를 학습하기 위해서 reparametrization trick을 이용한다는 것을 배웠습니다 (ref. Variational Autoencoder (VAE)).\n",
    "\n",
    "![reparam](images/reparam.png)\n",
    "\n",
    "Gumbel-softmax trick도 같은 맥락으로 이해할 수 있습니다. 그림으로 표현하면\n",
    "\n",
    "![gumbel_desc](images/gumbel_description.png)\n",
    "\n",
    "로 나타낼 수 있습니다. Stochastic node가 Gumbel random variable로 넘어가면서 class probability $(\\alpha_1, \\alpha_2, \\cdots, \\alpha_K)$ 자체도 학습을 할 수 있게 됩니다.\n",
    "\n",
    "지금까지 배운 내용을 토대로 dropout rate도 같이 학습할 수 있도록 실제 구현을 해보도록 하겠습니다. 들어가기에 앞서, 기본적인 컨셉은 다음과 같습니다.\n",
    "* 원래 dropout neural network를 구현하는 데는 tf.nn.dropout(dropout_probability)와 같은 형태의 dropout layer를 기존의 layer 사이에 끼워넣으면 된다.\n",
    "* **이를 dropout rate도 학습할 수 있는 concrete dropout layer를 만들어 기존의 dropout layer를 치환하도록 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 필요한 package들을 import하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.layers import base\n",
    "from tensorflow.python.layers import utils\n",
    "\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 tf.nn.dropout처럼 layer 사이에 추가만 하면 되는 concrete dropout layer를 만들어보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteDropout(base.Layer):\n",
    "    \"\"\"Concrete Dropout layer class from https://arxiv.org/abs/1705.07832.\n",
    "\n",
    "    \"Concrete Dropout\" Yarin Gal, Jiri Hron, Alex Kendall\n",
    "\n",
    "    Arguments:\n",
    "        weight_regularizer:\n",
    "            Positive float, satisfying $weight_regularizer = l**2 / (\\tau * N)$\n",
    "            with prior lengthscale l, model precision $\\tau$\n",
    "            (inverse observation noise), and N the number of instances\n",
    "            in the dataset.\n",
    "        dropout_regularizer:\n",
    "            Positive float, satisfying $dropout_regularizer = 2 / (\\tau * N)$\n",
    "            with model precision $\\tau$ (inverse observation noise) and\n",
    "            N the number of instances in the dataset.\n",
    "            The factor of two should be ignored for cross-entropy loss,\n",
    "            and used only for the eucledian loss.\n",
    "        init_min:\n",
    "            Minimum value for the randomly initialized dropout rate, in [0, 1].\n",
    "        init_min:\n",
    "            Maximum value for the randomly initialized dropout rate, in [0, 1],\n",
    "            with init_min <= init_max.\n",
    "        name:\n",
    "            String, name of the layer.\n",
    "        reuse:\n",
    "            Boolean, whether to reuse the weights of a previous layer\n",
    "            by the same name.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_regularizer=1e-6, dropout_regularizer=1e-5,\n",
    "                 init_min=0.1, init_max=0.1, name=None, reuse=False,\n",
    "                 training=True, **kwargs):\n",
    "\n",
    "        super(ConcreteDropout, self).__init__(name=name, _reuse=reuse,\n",
    "                                              **kwargs)\n",
    "        assert init_min <= init_max, \\\n",
    "            'init_min must be lower or equal to init_max.'\n",
    "\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        self.supports_masking = True\n",
    "        self.p_logit = None\n",
    "        self.p = None\n",
    "        self.init_min = (np.log(init_min) - np.log(1. - init_min))\n",
    "        self.init_max = (np.log(init_max) - np.log(1. - init_max))\n",
    "        self.training = training\n",
    "        self.reuse = reuse\n",
    "\n",
    "    def get_kernel_regularizer(self):\n",
    "        def kernel_regularizer(weight):\n",
    "            if self.reuse:\n",
    "                return None\n",
    "            return self.weight_regularizer * tf.reduce_sum(tf.square(weight)) \\\n",
    "                / (1. - self.p)\n",
    "        return kernel_regularizer\n",
    "\n",
    "    def apply_dropout_regularizer(self, inputs):\n",
    "        \"\"\"\n",
    "        위의 Evidence Lower BOund에서 H(p) = -p log p - (1 - p) log (1 - p)에 해당하는 부분입니다.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('dropout_regularizer'):\n",
    "            input_dim = tf.cast(tf.reduce_prod(tf.shape(inputs)[1:]),\n",
    "                                dtype=tf.float32)\n",
    "            dropout_regularizer = self.p * tf.log(self.p)\n",
    "            dropout_regularizer += (1. - self.p) * tf.log(1. - self.p)\n",
    "            dropout_regularizer *= self.dropout_regularizer * input_dim\n",
    "            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n",
    "                                 dropout_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape = tensor_shape.TensorShape(input_shape)\n",
    "        self.input_spec = base.InputSpec(shape=input_shape)\n",
    "\n",
    "        self.p_logit = self.add_variable(name='p_logit',\n",
    "                                         shape=[],\n",
    "                                         initializer=tf.random_uniform_initializer(\n",
    "                                             self.init_min,\n",
    "                                             self.init_max),\n",
    "                                         dtype=tf.float32,\n",
    "                                         trainable=True)\n",
    "        self.p = tf.nn.sigmoid(self.p_logit, name='dropout_rate')\n",
    "        tf.add_to_collection('DROPOUT_RATES', self.p)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def concrete_dropout(self, x):\n",
    "        eps = 1e-7\n",
    "        temp = 0.1\n",
    "\n",
    "        \"\"\"\n",
    "        Gumbel softmax trick이 여기서 들어가게 됩니다.\n",
    "        Bernoulli distribution의 continuous relaxation을 통해 sampling을 합니다.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('dropout_mask'):\n",
    "            unif_noise = tf.random_uniform(shape=tf.shape(x))\n",
    "            drop_prob = (\n",
    "                tf.log(self.p + eps)\n",
    "                - tf.log(1. - self.p + eps)\n",
    "                + tf.log(unif_noise + eps)\n",
    "                - tf.log(1. - unif_noise + eps)\n",
    "            )\n",
    "            drop_prob = tf.nn.sigmoid(drop_prob / temp)\n",
    "\n",
    "        with tf.name_scope('drop'):\n",
    "            random_tensor = 1. - drop_prob\n",
    "            retain_prob = 1. - self.p\n",
    "            x *= random_tensor\n",
    "            x /= retain_prob\n",
    "\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        def dropped_inputs():\n",
    "            return self.concrete_dropout(inputs)\n",
    "        if not self.reuse:\n",
    "            self.apply_dropout_regularizer(inputs)\n",
    "        return utils.smart_cond(training,\n",
    "                                dropped_inputs,\n",
    "                                lambda: array_ops.identity(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrete dropout layer를 조금 더 편리하게 사용할 수 있도록 하는 functional interface를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concrete_dropout(inputs,\n",
    "                     trainable=True,\n",
    "                     weight_regularizer=1e-6,\n",
    "                     dropout_regularizer=1e-5,\n",
    "                     init_min=0.1, init_max=0.1,\n",
    "                     training=True,\n",
    "                     name=None,\n",
    "                     reuse=False,\n",
    "                     **kwargs):\n",
    "\n",
    "    \"\"\"Functional interface for Concrete Dropout.\n",
    "\n",
    "    \"Concrete Dropout\" Yarin Gal, Jiri Hron, Alex Kendall\n",
    "    from https://arxiv.org/abs/1705.07832.\n",
    "\n",
    "    Arguments:\n",
    "        weight_regularizer:\n",
    "            Positive float, satisfying $weight_regularizer = l**2 / (\\tau * N)$\n",
    "            with prior lengthscale l, model precision $\\tau$\n",
    "            (inverse observation noise), and N the number of instances\n",
    "            in the dataset.\n",
    "        dropout_regularizer:\n",
    "            Positive float, satisfying $dropout_regularizer = 2 / (\\tau * N)$\n",
    "            with model precision $\\tau$ (inverse observation noise) and\n",
    "            N the number of instances in the dataset.\n",
    "            The factor of two should be ignored for cross-entropy loss,\n",
    "            and used only for the eucledian loss.\n",
    "        init_min:\n",
    "            Minimum value for the randomly initialized dropout rate, in [0, 1].\n",
    "        init_min:\n",
    "            Maximum value for the randomly initialized dropout rate, in [0, 1],\n",
    "            with init_min <= init_max.\n",
    "        name:\n",
    "            String, name of the layer.\n",
    "        reuse:\n",
    "            Boolean, whether to reuse the weights of a previous layer\n",
    "            by the same name.\n",
    "\n",
    "    Returns:\n",
    "        Tupple containing:\n",
    "            - the output of the dropout layer;\n",
    "            - the kernel regularizer function for the subsequent\n",
    "              convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    layer = ConcreteDropout(weight_regularizer=weight_regularizer,\n",
    "                            dropout_regularizer=dropout_regularizer,\n",
    "                            init_min=init_min, init_max=init_max,\n",
    "                            training=training,\n",
    "                            trainable=trainable,\n",
    "                            name=name,\n",
    "                            reuse=reuse)\n",
    "    return layer.apply(inputs, training=training), \\\n",
    "        layer.get_kernel_regularizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 실제 동작을 위한 모든 부분을 완성하였습니다. 앞서 언급했듯이, 기존의 dropout layer를 방금 완성한 concrete dropout layer로 대체하기만 하면 됩니다!! \n",
    "\n",
    "이제 MNIST dataset에 대해 dropout rate도 실제로 학습이 되는지 보도록 하겠습니다.\n",
    "다음은 Layer를 위한 추가적인 module을 import하고 Convolutional neural network를 선언하는 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(inputs, is_training):\n",
    "\n",
    "    x = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "\n",
    "    dropout_params = {'init_min': 0.1, 'init_max': 0.1,\n",
    "                      'weight_regularizer': 1e-6, 'dropout_regularizer': 1e-5,\n",
    "                      'training': is_training}\n",
    "    x, reg = concrete_dropout(x, name='conv1_dropout', **dropout_params)\n",
    "    x = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu, padding='SAME',\n",
    "                   kernel_regularizer=reg, bias_regularizer=reg,\n",
    "                   name='conv1')\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2, padding='SAME', name='pool1')\n",
    "\n",
    "    x, reg = concrete_dropout(x, name='conv2_dropout', **dropout_params)\n",
    "    x = tf.layers.conv2d(x, 64, 5, activation=tf.nn.relu, padding='SAME',\n",
    "                   kernel_regularizer=reg, bias_regularizer=reg,\n",
    "                   name='conv2')\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2, padding='SAME', name='pool2')\n",
    "\n",
    "    x = tf.reshape(x, [-1, 7*7*64], name='flatten')\n",
    "    x, reg = concrete_dropout(x, name='fc1_dropout', **dropout_params)\n",
    "    x = tf.layers.dense(x, 1024, activation=tf.nn.relu, name='fc1',\n",
    "                  kernel_regularizer=reg, bias_regularizer=reg)\n",
    "\n",
    "    outputs = tf.layers.dense(x, 10, name='fc2')\n",
    "    return outputs\n",
    "\n",
    "def net2(inputs, is_training):\n",
    "    \"\"\"\n",
    "    위의 net 함수의 경우는 convolutional neural network를 나타냅니다.\n",
    "    32 channels --> 64 channels --> 3136 (7*7*64) hidden units --> 1024 hidden units --> 10 units for class 0 ~ 9\n",
    "    의 구조를 갖는 CNN architecture입니다.\n",
    "    \n",
    "    net2 함수에서는 hidden layer 2개를 갖는 fully connected models을 구현해보도록 합니다.\n",
    "    두 개의 hidden layer는 각각 300개와 100개의 neuron을 가지도록 합니다.\n",
    "    \n",
    "    즉, 구현해야 할 fully connected models의 구조는\n",
    "    784 input dimension --> 300 hidden units --> 100 hidden units --> 10 units for class 0 ~ 9\n",
    "    입니다.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 MNIST 학습을 위한 모델을 만드는 코드였습니다. 이제 실제로 학습하는 code입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    # MNIST dataset load\n",
    "    mnist = input_data.read_data_sets('MNIST_data')\n",
    "\n",
    "    # MNIST has 28x28 = 784 input dimension and 10 classes\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    y_out = net(x, is_training)\n",
    "    \"\"\"\n",
    "    y_out의 경우 현재 CNN을 통해 predict된 label을 의미합니다.\n",
    "    위의 net2로 구현한 Fully-connected model에 대해서도 확인해보도록 합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # softmax loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(\n",
    "                labels=y, logits=y_out))\n",
    "        loss += tf.reduce_sum(\n",
    "                tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "    # optimizer we will use\n",
    "    with tf.name_scope('adam_optimizer'):\n",
    "        train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "    # to keep track of model accuracy\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_out, 1), y)\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "    # dropout rates for all layers\n",
    "    dropout_rates = tf.get_collection('DROPOUT_RATES')\n",
    "    def rates_pretty_print(values):\n",
    "        return {str(t.name): round(r, 4)\n",
    "                for t, r in zip(dropout_rates, values)}\n",
    "\n",
    "    # actual running part!!\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # How many iterations?\n",
    "        for i in range(5000):\n",
    "            # Mini-batch size!\n",
    "            batch = mnist.train.next_batch(50)\n",
    "            if i % 500 == 0:\n",
    "                training_loss, training_acc, rates = sess.run(\n",
    "                        [loss, accuracy, dropout_rates],\n",
    "                        feed_dict={\n",
    "                            x: batch[0], y: batch[1], is_training: False})\n",
    "                print('step {}, loss {}, accuracy {}'.format(\n",
    "                    i, training_loss, training_acc))\n",
    "                print('dropout rates: {}'.format(rates_pretty_print(rates)))\n",
    "            train_step.run(feed_dict={\n",
    "                x: batch[0], y: batch[1], is_training: True})\n",
    "\n",
    "        accuracy, rates = sess.run([accuracy, dropout_rates],\n",
    "                                   feed_dict={x: mnist.test.images,\n",
    "                                              y: mnist.test.labels,\n",
    "                                              is_training: False})\n",
    "        print('test accuracy {}'.format(accuracy))\n",
    "        print('final dropout rates: {}'.format(rates_pretty_print(rates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.run(main=main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 dropout rate가 각 layer별로 다르게 학습되는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습 문제 (Hint는 코드 및 코드의 주석을 참고해주세요!)\n",
    "\n",
    "**Q1.** iteration 횟수를 10000번, 20000번, 40000번으로 나눠서 학습을 진행해보세요.\n",
    "\n",
    "**Q2.** mini-batch size를 100, 200에 대해서 학습을 진행해보세요.\n",
    "\n",
    "**Q3 (Miscellaneous).** 지금은 convolutional layer에 대해서 학습을 해보았는데 hidden layer가 2개이며 각각이 300개와 100개의 unit을 갖는 fully-connected model에 대해서 학습을 진행해보세요 (즉, 784 (MNIST input dimension) - 300 - 100 - 10 (output dimension)의 structure를 갖는 모델입니다). --> 함수 net2 부분을 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
